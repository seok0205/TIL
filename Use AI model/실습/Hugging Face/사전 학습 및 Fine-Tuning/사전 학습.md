# 사전 학습

- 대규모의 텍스트 데이터셋을 사용해 모델이 일반적인 언어 이해능력을 학습하는 과정이다. 이 단계에서는 특정 작업(ex. 번역, 감정 분석)을 염두에 두지 않고, 단순히 언어의 패턴, 구조를 학습하는 것이 목적.  

## 특징

1. __대규모 데이터셋 사용__
    - 인터넷에서 수집한 방대한 양의 텍스트 데이터로 모델 학습. 예로, BERT는 수십억 개의 문장으로 사전 학습되었음.

2. __일반적인 언어 이해__
    - 모델은 텍스트 내 단어의 의미, 문장 구조, 문맥 등 언어의 전반적 특징을 학습함.

3. __작업 비특화__
    - 특정 작업에 맞춰진 학습이 아닌, 전반적 언어 이해에 초점.

## 목적

- 모델은 다양한 텍스트에서의 언어의 기본적 규칙을 배움. 이후 특정 작업에 빠르게 적응할 수 있는 기반을 다진다. Hugging Face에서 제공하는 대부분의 모델들은 이 단계까지 완료된 상태로 제공.

## BERT의 사전 학습

- BERT는 사전 학습에서 두 가지 주요 작업을 수행.

1. __Masked Language Modeling__ (MLM)  
    - 문장의 일부 단어를 마스킹(masking)한 후, 이를 예측하도록 모델을 학습. 이 과정 통해 BERT는 문맥을 양방향으로 이해 가능.

2. __Next Sentence Prediction__ (NSP)  
    - 두 문장이 주어졌을 때, 두 번째 문장이 첫 번째 문장 뒤에 자연스럽게 이어지는지를 예측한다. 이를 통해 문장 간의 관계를 이해하는 능력을 학습한다.
