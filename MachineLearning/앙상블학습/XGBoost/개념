XGBoost - eXtreme Gradient Boosting

GBM알고리즘을 기반으로 하는 고성능 앙상블 학습 기법.
효율성, 유연성, 이식성을 목표로 설계, 다양한 머신러닝 경진대회에서 우수한 성능 보여줌.
[병렬 처리, 조기종료, 정규화] 등의 기능을 통해 성능 극대화

구조 :
XGBoost는 여러 개의 결정 트리(Decision Tree)로 구성.
각 결정 트리는 이전 트리의 예측 오류를 보완하는 방식으로 학습된다.
각 트리의 예측 결과를 가중합하여 최종 예측 수행.

원리 :
1. 첫 번째 결정 트리를 학습시켜 초기 모델 만듦.
2. 초기 모델의 예측 결과와 실제 값 간의 잔여 오차를 계산.
3. 잔여 오차를 예측하는 새로운 결정 트리를 학습시킨다.
4. 새로운 결정 트리를 기존 모델에 추가해 모델을 업데이트.
5. 잔여 오차 충분히 줄어들 때까지 2~4단계 반복.

장점 :
병렬 처리 - 트리의 분할을 병렬로 수행하여 학습 속도 향상.
조기 종료 - 검증 데이터셋의 성능이 향상되지 않으면 학습을 조기에 종료.(과적합 방지)
정규화 - L1, L2 정규화를 통해 모델의 복잡도 조절, 과적합 방지.
유연성 - 다양한 손실 함수와 평가 지표 지원해 다양한 문제에 적용 가능.