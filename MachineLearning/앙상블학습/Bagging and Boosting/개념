Bagging

여러 개의 학습 모델을 병렬로 학습시키고, 그 예측 결과를 평균 or 다수결로 결합하는 기법.
데이터 샘플링 과정에서 부트스트래핑(bootstrap)기법 사용. 원본 데이터셋에서 중복을 허용한 무작위 샘플을 생성.
각 모델은 서로 다른 데이터 샘플을 학습. 모델 간 상관성 줄이고 예측 성능 향상시킴.

장점 :
여러 모델의 예측을 결합함으로써 과적합을 줄임.
데이터의 변동에 덜 민감해진다.(안정성)
각 모델을 독립적으로 학습시킬 수 있어 병렬 처리가 가능하다.

Boosting

여러 개의 약한 학습기(weak learner)를 순차적으로 학습, 예측 결과를 결합해 강한 학습기(strong learner)를 만드는 기법.
이전 모델이 잘못 예측한 데이터 포인트에 가중치를 부여, 다음 모델이 이를 더 잘 학습하도록 함.

장점 :
약한 학습기를 결합해 높은 예측 성능 얻을 수 있음.
모델의 복잡도를 조절해 과적합을 방지할 수 있음.
이전 모델의 오류를 보완하는 방식으로 학습이 진행됨.

*부스팅 모델은 후에 GBM에서 예제 코드 다룸.