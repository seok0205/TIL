Random Forest

Bagging 기법을 기반으로 한 앙상블 학습이다.
여러 개의 Decision Tree(결정 트리)를 학습시키고, 그 예측 결과를 결합해 최종 예측을 수행.
각 트리가 독립적으로 학습하기 때문에, 과적합 방지와 예측 성능을 향상시킬 수 있다.

구조 :
여러 개의 결정 트리(Decision Tree)로 구성.
각 결정 트리는 데이터의 무작위 샘플을 사용해 학습.
트리의 예측 결과를 평균 or 다수결로 결합해 최종 예측 수행.

원리 :
부트스트랩(bootstrap) 샘플링 - 원본 데이터셋에서 중복을 허용한 무작위 샘플 생성.
각 부트스트램 샘플을 사용해 결정 트리 학습. 이때, 각 노드에서 무작위로 선택된 특성의 일부만 사용해 분할 수행.
모든 결정 트리의 예측 결과를 결합하여 최종 예측 수행. 회귀 문제에서는 평균, 분류에서는 다수결 사용.

무작위성 :
두 가지 무작위성을 도입해 모델의 다양성 증가, 과적합 방지.

1. 데이터 샘플링의 무작위성 :
각 결정 트리는 원본 데이터셋에서 무작위로 샘플링된 데이터로 학습됨.

2. 특성 선택의 무작위성 :
각 노드에서 분할을 수행할 때, 무작위로 선택된 특성의 일부만을 사용.

이러한 무작위성은 모델의 상관성을 줄이고, 예측 성능을 향상시킴.