Attention 메커니즘

시퀀스 데이터에서 중요한 부분에 더 많은 가중치를 할당해 정보 효율적으로 처리.
주로 NLP와 시계열 데이터에서 사용, 기계 번역, 요약, 질의응답 시스템 등 성능 발휘.

동작 방식 :
    1. 개요 -
        입력 시퀀스의 각 요소에 대해 중요도 계산, 가중치 부여
        중요한 정보에 집중, 불필요한 정보 무시.
        Query, Key, Value 세가지 구성 요소.

    2. Attention score 계산
        Query, Key 간의 유사도를 측정하여 중요도 계산.
        이 유사도는 내적(dot product) 등을 사용해 계산.
    
    3. Softmax를 통한 가중치 계산
        계산된 Attention score는 softmax 함수를 통해 확률 분포로 변환.
        이를 통해 가중치의 합이 1이 되도록 함.

    4. Softmax를 통한 가중치 계산
        Softmax를 통해 얻어진 가중치를 Value에 곱하여 최종 Attention 출력을 계산.

Self-Attention :
    시퀀스 내의 각 요소가 서로를 참조하는 메커니즘. 입력 시퀀스의 모든 요소가 Query, Key, Value로 사용.
    이를 통해 각 요소가 시퀀스 내 다른 요소들과의 관계를 학습할 수 있음.
    예를 들어서, 문장 내에서 단어 간의 관계를 학습하여 번역이나 요약에 활용.

Multi-Head Attention :
    여러개의 Self-Attention을 병렬로 수행하는 메커니즘.
    각 헤드는 서로 다른 부분의 정보를 학습, 이를 통해 모델이 다양하나 관점에서 데이터를 처리.