{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 훈련 데이터 예시: ['thisisanegativereview', 'thisisanegativereview', 'thisisanegativereview', 'thisisanegativereview', 'thisisapositivereview']\n",
      "전처리된 검증 데이터 예시: ['thisisanegativereview', 'thisisanegativereview', 'thisisapositivereview', 'thisisapositivereview', 'thisisanegativereview']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "# 가짜 데이터 생성\n",
    "np.random.seed(42)\n",
    "num_samples = 1000\n",
    "texts = [\"This is a positive review.\" if i % 2 == 0 else \"This is a negative review.\" for i in range(num_samples)]\n",
    "labels = [1 if i % 2 == 0 else 0 for i in range(num_samples)]\n",
    "\n",
    "# 데이터 분할\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # 소문자 변환\n",
    "    text = re.sub(r'[^a-z0-9\\\\s]', '', text)  # 특수 문자 제거\n",
    "    return text\n",
    "\n",
    "# 전처리 적용\n",
    "train_texts = [preprocess_text(text) for text in train_texts]\n",
    "val_texts = [preprocess_text(text) for text in val_texts]\n",
    "\n",
    "# 전처리된 데이터 확인\n",
    "print(\"전처리된 훈련 데이터 예시:\", train_texts[:5])\n",
    "print(\"전처리된 검증 데이터 예시:\", val_texts[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 및 패딩된 훈련 데이터 예시:\n",
      "tensor([[  101,  2023, 29196, 29107,  6024,  2890,  8584,   102,     0,     0],\n",
      "        [  101,  2023, 29196, 29107,  6024,  2890,  8584,   102,     0,     0],\n",
      "        [  101,  2023, 29196, 29107,  6024,  2890,  8584,   102,     0,     0],\n",
      "        [  101,  2023, 29196, 29107,  6024,  2890,  8584,   102,     0,     0],\n",
      "        [  101,  2023, 14268,  6873, 28032, 16402,  6777,  2666,  2860,   102]])\n",
      "토큰화 및 패딩된 검증 데이터 예시:\n",
      "tensor([[  101,  2023, 29196, 29107,  6024,  2890,  8584,   102,     0,     0],\n",
      "        [  101,  2023, 29196, 29107,  6024,  2890,  8584,   102,     0,     0],\n",
      "        [  101,  2023, 14268,  6873, 28032, 16402,  6777,  2666,  2860,   102],\n",
      "        [  101,  2023, 14268,  6873, 28032, 16402,  6777,  2666,  2860,   102],\n",
      "        [  101,  2023, 29196, 29107,  6024,  2890,  8584,   102,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# BERT 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 텍스트 토큰화 및 패딩\n",
    "def tokenize_and_pad(texts, tokenizer, max_len):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_len, return_tensors='pt')\n",
    "    return encodings\n",
    "\n",
    "max_len = 20\n",
    "train_encodings = tokenize_and_pad(train_texts, tokenizer, max_len)\n",
    "val_encodings = tokenize_and_pad(val_texts, tokenizer, max_len)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"토큰화 및 패딩된 훈련 데이터 예시:\")\n",
    "print(train_encodings['input_ids'][:5])\n",
    "print(\"토큰화 및 패딩된 검증 데이터 예시:\")\n",
    "print(val_encodings['input_ids'][:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Seok",
   "language": "python",
   "name": "seok"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
