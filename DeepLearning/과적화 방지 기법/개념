정규화 - Normalization :
    입력 데이터의 분포를 일정한 범위로 조정, 모델의 학습을 안정화하고 성능을 향상시키는 기법.

    배치 정규화(Batch Normalization) :
        각 미니배치의 평균과 분산을 사용하여 정규화.
        이는 학습 속도를 높이고, 과적합 방지에 도움이 됨.
    
    레이어 정규화(Layer Normalization) :
        각 레이어의 뉴런 출력을 정규화.

드롭아웃 - Dropout :
    학습 과정에서 무작위로 뉴런을 비활성화하여, 모델의 과적합 방지하는 기법.
    학습 시에만 적용, 평가 시에는 모든 뉴런 활성화.

조기 종료 - Early Stopping :
    검증 데이터의 성능이 더 이상 향상되지 않을 때 학습을 중단하여, 과적합 방지.
    조기 종료는 학습 과정에서 검증 손실이 일정 에포크 동안 감소하지 않으면 학습을 중단.

데이터 증강 - Data Augmentation :
    원본 데이터를 변형하여 새로운 데이터를 생성, 데이터셋을 확장하고 모델의 일반화 성능 향상.
    회전, 이동, 크기 조절, 색상 변환 등.