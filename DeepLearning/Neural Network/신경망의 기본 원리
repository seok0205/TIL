퍼셉트론(Perceptron) :
    인공신경망의 기본 단위. 뉴런을 모델링한 것.
    퍼셉트론은 입력 값을 받아 가중치를 곱하고, 이를 모두 더한 후 활성화 함수(activation function)를 통해 출력 값을 결정.

다층 퍼셉트론(MLP) :
    Multi-Layer Perceptron(다층 퍼셉트론)은 여러 층의 퍼셉트론을 쌓아 올린 신경망 구조.
    입력층(input layer), 은닉층(hidden layer), 출력층(output layer)으로 구성. 각 층의 뉴런들은 서로 연결.

    input layer - 
    외부 데이터가 신경망에 입력되는 부분. 입력 레이어의 뉴런 수는 입력 데이터의 특징 수와 동일.

    hidden layer - 
    입력과 출력 레이어 사이에 위치. 입력 데이터를 처리하고 특징을 추출하는 역할.
    은닉 레이어의 뉴런 수와 층 수는 모델의 복잡성과 성능에 영향을 미침.

    output layer -
    신경망의 마지막 층. 최종 예측 값 출력. 출력 레이어의 뉴런 수는 예측하려는 클래스 수 or 회귀 문제의 출력 차원과 동일.

XOR 문제, MLP :
    단일 퍼셉트론은 선형 분류, 따라서 XOR 같은 비선형 문제 해결 불가.
    XOR 문제는 두 입력값이 다를 때만 1을 출력. 단일 퍼셉트론으로는 해결 X.
    MLP는 은닉층을 통해 비선형성을 학습 가능. 따라서 XOR 문제 해결 가능.

활성화 함수 :
    신경망의 각 뉴런에서 입력값을 출력값으로 변환하는 역할.
    비선형성 도입, 복잡한 패턴 학습 가능.
    활성화 함수가 없다면 신경망은 단순 선형변환만 수행, 복잡한 패턴 학습 불가.

활성화 함수 종류 :
    ReLU(Rectified Linear Unit) -
        장점 - 계산 간단, 기울기 소실 문제(vanishing gradient problem)를 완화.
        단점 - 음수 입력에 대해 기울기가 0이 되는 '죽은ReLU' 문제가 발생할 수 있다.
    
    Sigmoid -
        장점 - 출력 값 0과 1 사이로 제한. 확률 표현에 적합.
        단점 - 기울기 소실 문제와 출력 값이 0과 1에 가까워질 때 학습이 느려짐.
    
    Tanh(Hyperbolic Tangent) -
        장점 - 출력 값이 -1과 1 사이 제한. 중심이 0에 가까워짐.
        단점 - 기울기 소실 문제 발생.

손실함수 :
    모델의 예측 값과 실제 값 사이의 차이를 측정하는 함수.
    모델 성능 평가, 최적화 알고리즘 통해 모델을 학습시키는 데 사용.

손실함수 종류 :
    MSE(Mean Squared Error) -
        회귀 문제에서 주로 사용. 예측 값과 실제 값의 차이를 제곱해서 평균을 구함.
    Cross-Entropy -

        분류 문제에서 주로 사용. 예측 확률과 실제 클래스 간의 차이를 측정.

최적화 알고리즘 :
    손실 함수를 최소화하기 위해 모델의 가중치를 조정하는 방법.
    손실 함수의 기울기를 계산, 이를 바탕으로 가중치 업데이트.

최적화 알고리즘 종류 :
    SGD - 
        무작위로 샌택된 일부 데이터(mini batch)를 사용하여 기울치 계산, 업데이트.
        장점 - 계산이 빠르고 큰 데이터셋에서 효율적으로 동작.
        단점 - 최적점에 도달하기전까지 진동 발생 가능.
    Adam(Adaptive Moment Estimation) -
        모멘텀 RMSProp을 결합한 알고리즘, 학습률을 적용적으로 조정.
        빠른 수렴 속도와 안정적인 학습 제공.
        하이퍼파라미터 설정이 복잡할 수가 있다.

역전파(Backpropagation) :
    신경망의 가중치를 학습시키기 위해 사용되는 알고리즘.

    원리 :
    연쇄 법칙(Chain Rule)을 사용해 손실함수의 기울기 계산.
    각 층의 기울기는 이전 충의 기울기와 현재 층의 기울기를 곱하여 계산한다.