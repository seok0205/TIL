# concept of LLM

## LLM

- LLM(Large Language Model)(대형 언어 모델) :  
    대규모 텍스트 데이터를 학습하여 자연어를 이해하고 생성할 수 있는 수십억 개의 파라미터를 기반으로 한 AI 모델. 기본적으로 자연어 처리(NLP)의 다양한 작업, 예로 번역, 질문 응답, 텍스트 생성 등을 할 수 있다.

### 동작 원리

1. 학습(Training)  
    LLM은 대규모 텍스트 데이터셋을 이용해 학습.  
    __패턴 인식__ : 수많은 텍스트에서 단어와 문장의 패턴을 찾아내어, 새 문장이나 답변을 생성할 때 패턴을 적용.

2. 추론(Inference)  
    학습된 LLM은 질문이나 입력을 받으면, 그에 맞는 추론을 통해 답변을 생성. 이때, 이전의 맥락을 기억하고 활용하면서 답을 만듦.

3. 미세 조정(Fine-tuning)  
    LLM은 특정 도메인이나 용도에 맞춰 fine-tuning을 할 수 있다. 예로 의료나 법률 같은 특수한 분야에 맞는 데이터를 추가로 학습시키면 해당 분야에 대한 답변의 정확성이 높아진다.

### 랜덤성과 조건성

1. 랜덤성(Randomness)

    - 기본적으로 확률에 기반하여 문장을 생성. 따라서 같은 질문에도 같은 답변이 나오지 않을 수도 있다. 새로운 문장을 만들어내는 능력을 키우는 핵심 요소.
    - LLM이 결과를 생성할 때, 토큰의 확률 분포를 계사ㅏ나해 그 중 높은 확률을 가진 토큰을 선택해 문장을 만듦. 이때 'temperature'라는 매개변수가 랜덤성에 영향을 미침. 이 매개변수가 낮으면 일관성이 있고 예측이 가능한 답변을 생성하고, 높으면 창의적이고 예측이 어려운 결과가 나올 수 있다. 각각 랜덤성이 더 적고, 많다고 말할 수 있다.

2. 조건성(Conditioning)

    - LLM은 조건부 확률을 기반으로 결과를 도출. 즉, 모델은 이전 입력 내용에 따라 문장을 조건부로 생성하게 되는데, 이를 context라고 한다.
    - 위 과정에서 두 가지의 중요 요소가 있는데, 첫 번째는 입력된 문자아이나 질문이 무엇인지에 따라 결과가 달라지는 프롬프트, 두 번째로는 대화를 나누는 동안 이전 문장이나 대화 흐름을 기억하고 그에 맞춘 답을 생성하는 맥락 기억이 있다.

### LLM의 원리 요약

- 원리를 이해하면 정확한 프롬프트 작성이 가능해져서 원하는 답을 쉽게 얻을 수 있고, LLM의 한계를 안다면 적절한 상황에서의 도구 선택을 이끌어낼 수 있다. 또한 RAG나 LangChain 같은 성능 개선을 위한 기술 요소들도 효과적으로 도입할 수 있다.

1. 대규모 데이터 학습 : 언어 패턴 학습

2. 문맥 기반 추론 : 텍스트 맥락 파악으로 답 도출

3. 랜덤성 및 조건성 : 확률 기반 답변 생성, 조건에 따라 결과 상이

4. Fine-tuning : 특정 용도나 도메인에 맞춰 학습 가능

### 추가 개념

1. RAG(Retrieval-Augmented Generation) :  
    LLM이 스스로 모든 답을 생성하는 대신에 데이터베이스에서 정보를 검색한 것을 기반으로 답변을 생성하는 기법이고, LLM의 한계를 극복할 수 있는 방법 중 하나이다.

2. Vector DB :  
    텍스트를 벡터 형태로 변환하여 유사한 의미를 가진 텍스트를 효율적으로 검색할 수 있게 도와주는 기술. LLM과 결합 시 강력한 검색, 응답 기능을 구현 가능하다.

## LLM 시스템 구축에 필요한 네 가지 핵심 요소

### 1. LLM(Large Language Model)

- 방대한 양의 텍스트 데이터를 학습해 자연어를 이해하고 생성할 수 있는 AI. 이 모델들은 큰 파라미터를 가지고 있고, 텍스트의 Context를 파악해 다양한 언어 작업을 수행 가능.

- ex. GPT-3, GPT-4, BERT

#### LLM 주요 기능

1. __자연어 이해(NLU)__ : 질문, 명령 이해하고 적절히 응답하는 능력.
2. __텍스트 생성__ : 기존 데이터 바탕, 자연스러운 텍스트 생성.
3. __번역 및 요약__ : 다른 언어로 번역, 긴 텍스트를 요약.
4. __질문 응답 시스템(Q&A)__ : 사용자 질문에 정확히 답변하는 기능.

- 다양한 곳에 응용되지만 모든 답변을 자체적으로 생성하는 데는 한계가 있어, 이를 보완하는 기법들이 필요함.

#### ChatGPT의 문제점

- 이루다 사태가 대표적인 예. 폐쇄형 LLM인 GPT-4o의 경우 질문이나 답변 등이 회사 서버에 저장됨. 해당 질문 답변을 AI에게 재학습시키기도 하기 때문에, 나중에 다른 사람의 채팅에서 민감한 정보가 포함될 가능성이 있음.

- 위의 예를 제외해도, ChatGPT에 회사 DB 비밀번호를 붙여넣는다던지, 삼성전자에서 기밀문서를 GPT에 입력했다는 등 여러 문제들이 나타남. 기업들이 내부의 정보를 활용한 챗봇 등을 만들고 싶다면 데이터 유출 등의 보안 문제를 해결해야함.

### 2. RAG(Retrieval-Augmented Generation)

- __검색 증강 생성 기법__, LLM은 많은 데이터를 학습했지만, 최신 정보나 특정 도메인 지식에는 한계를 가질 수 있다. 이를 보완하기 위해 RAG는 LLM이 직접 답을 생성하는 대신, 외부 데이터베이스나 문서에서 관련 정보를 검색하여 그 정보를 바탕으로 답변을 생성하는 방식이다.

#### RAG의 동작 원리

1. __질문 입력__ : 사용자가 질문을 하면, RAG 시스템은 질문에 맞는 답변을 생성하기 전에 검색 단계를 거친다.

2. __문서 검색(Retrieval)__ : 벡터 DB나 기타 정보 저장소에서 질문의 텍스트를 벡터화하여 의미적으로 유사하거나 관련된 문서를 검색.

3. __답변 생성(Generation)__ : 검색된 문서를 바탕으로 LLM이 최종적으로 답변 생성.

#### RAG의 장점

1. __최신 정보 활용__ : LLM은 학습된 데이터가 오래될 수 있음. RAG는 최신 데이터베이스에서 정보 검색.

2. __특정 도메인 정보 제공__ : 도메인에 특화된 정보 제공 가능. 일반적 LLM보다 정확한 정보 제공.

3. __효율성__ : 필요한 정보만 검색. LLM의 모든 지식을 외부에 의존하지 않고도 효율적 사용 가능.

#### RAG의 예시

- 법률이나 의료와 같은 특수 분야에서 질문이 들어올 경우, LLM은 법률, 의료 데이터베이스에서 관련 문서를 검색하고, 이를 바탕으로 답변 구성 가능. LLM 기본 모델에 의존하지 않고도, 실시간으로 정확하고 최신의 정보를 제공함.

- AnythingLLM을 통해 GUI 툴로 RAG를 간단히 해볼 수 있음. RAG 워크스페이스를 API로 내보내는 것도 가능함. 민감하지 않은 정보를 다룰거라면 OpenAI 등의 폐쇄형 LLM도 가능하다.

### 3. Vector DB (벡터 데이터베이스)

- 텍스트, 이미지 등의 데이터를 '__벡터 형태__'로 변환해 저장. 그 벡터를 기반으로 데이터를 빠르고 효율적으로 검색하는 데이터 베이스. '__임베딩(embedding)__'이라는 방법으로 데이터를 벡터화하여, 유사한 의미를 가진 데이터들을 빠르게 검색할 수 있게 해준다.

- __임베딩(embedding)__ :  
    텍스트나 이미지를 수학적으로 __벡터__(숫자 배열)로 변환하는 과정.  
    예로, '강아지'라는 단어를 벡터로 변환하면 그 벡터는 '고양이'와 같은 다른 동물과도 유사한 벡터값을 가질 수 있음.  
    벡터 DB는 이러하나 임베딩된 데이터를 기반으로 문서 검색을 수행.

#### Vector DB의 동작 과정

1. __임베딩 생성__ : 문서나 텍스트를 벡터로 변환. 이 벡터는 텍스트의 의미적 정보를 담음.

2. __벡터 저장__ : 생성된 벡터를 DB에 저장.

3. __벡터 검색__ : 검색어 입력 시, 해당 검색어를 벡터로 변환, DB에서 유사한 벡터 찾음.

4. __결과 제공__ : 유사한 벡터를 가진 문서나 데이터를 검색 결과로 제공.

### Vector DB의 장점

1. __의미 기반 검색__ : 단순 키워드 매칭 X. 텍스트의 의미에 기반한 검색 가능, 유사한 의미를 가진 텍스트도 검색 가능.

2. __고성능 처리__ : 대량의 벡터 데이터를 매우 빠르게 처리 가능, 대규모 텍스트 데이터에 대해 효율적 검색이 가능.

#### Vector DB의 활용 사례

1. 문서 검색 시스템 : 대규모 문서 저장소에서 특정 주제에 대한 유사한 문서 빠르게 검색.

2. 이미지 검색 : 이미지를 벡터화, 유사한 이미지나 관련 이미지를 빠르게 검색.

3. 질문-답변 시스템 : 질문에 대한 관련 정보를 벡터 기반으로 검색해 답변 제공.

### 4. LangChain

- LLM과 같은 언어 모델을 더욱 효율적으로 활용할 수 있게 도와주는 framework.
- 목적은 다양한 LLM과 외부 리소스를 결합해 강력한 언어 기반 애플리케이션을 만들 수 있도록 돕는 것.
- LLM의 기능을 더욱 확장하고, 데이터 소스, API, DB 등을 쉽게 통합할 수 있다.

#### LangChain 주요 기능

1. __프롬프트 체인(Prompt Chains)__ :  
    여러 단계의 프롬프트를 '__연속적으로 연결__'하여 복잡한 작업을 수행할 수 있음.  예로, 먼저 문서를 요약하고, 그 내용을 바탕으로 질문에 답하는 구조 만들 수 있음.

2. __메모리 기능__ :  
    LLM은 보통 이전 대화를 기억하지 못함. LangChain은 메모리 기능을 통해 대화의 맥락을 유지할 수 있게 해줌.

3. __외부 리소스 통합__ :  
    API, DB, 웹 검색 등 다양한 외부 리소스를 결합해 LLM의 한계를 보완할 수 있도록 함.

#### LangChain의 장점

1. __유연한 애플리케이션 개발__ : 복잡한 언어 작업을 자동화하거나 여러 단계의 작업을 간단히 연결 가능.

2. __확장성__ : LLM이 처리할 수 없는 작업을 외부 리소스를 통해 보완하여 더욱 강력한 AI 시스템 구축 가능.

3. __대화형 AI__ : 장기적인 대화 흐름 관리 가능. 챗봇 or 고급 대화 시스템 제작에 유리.

#### LangChain의 활용 사례

1. 자동화된 텍스트 처리 : 문서 요약, 분석, 번역, QA시스템 등 여러 텍스트 처리 작업을 하나의 체인으로 연결.

2. 대화형 에이전트 : LangChain을 사용해 사용자와 지속적 대화를 유지하는 챗봇이나 대화형 AI 시스템을 구축.

3. 복잡한 워크플로우 처리 : LLM과 외부 데이터 소스를 결합해 복잡한 워크플로우를 자동화하고 처리.
