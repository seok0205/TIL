{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Langchain, FAISS\n",
    "\n",
    "- LangChain 사용 위한 환경 설정과 FAISS로 Vector DB를 구성하는 실습\n",
    "\n",
    "## 설치 및 기본 설정\n",
    "\n",
    "'pip install langchain langchain-openai faiss-cpu'  \n",
    "  \n",
    "- LangChain, OpenAI, FAISS 패키지 설치.  \n",
    "  \n",
    "- 설치 후, OpenAI API 키 설정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path='key.env')\n",
    "api_key=os.getenv(\"GROUP_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain 기본 개념\n",
    "\n",
    "1. __언어 모델 초기화__\n",
    "\n",
    "- GPT-4 모델을 LangChain 통해 사용. ChatOpenAI를 이용해 초기화, invoke 메서드로 메시지 전달해 응답 받아옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 어떤 부분에서 도움이 필요하신지 알려주시면 제가 도와드리도록 하겠습니다. 정보 검색, 일정 관리, 알람 설정 등 다양한 부분에서 도움을 드릴 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 모델 초기화\n",
    "model = ChatOpenAI(model=\"gpt-4\", api_key=api_key)\n",
    "\n",
    "# 모델에 메시지 전달\n",
    "response = model.invoke([HumanMessage(content=\"안녕하세요, 무엇을 도와드릴까요?\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __프롬프트 템플릿 사용하기__\n",
    "\n",
    "- 다양한 입력을 받아 메시지 생성에 도움을 줌. 영어 문장을 다른 언어로 번역하는 프롬프트 정의."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='Translate the following sentence from English to Korean:', additional_kwargs={}, response_metadata={}), HumanMessage(content='Which team is the best football club in the world?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 시스템 메시지 설정\n",
    "system_template = \"Translate the following sentence from English to {language}:\"\n",
    "\n",
    "# 사용자 텍스트 입력\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"user\", \"{text}\")\n",
    "])\n",
    "\n",
    "# 프롬프트 생성\n",
    "result = prompt_template.invoke({\"language\": \"Korean\", \"text\": \"Which team is the best football club in the world?\"})\n",
    "print(result.to_messages())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Expression Language(LCEL)로 체인 연결\n",
    "\n",
    "- 여러 컴포넌트를 체인으로 연결해서 데이터 흐름을 통제하는 LCEL 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "あなたの鉛筆はどこですか？\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 응답을 파싱하는 파서 초기화\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 템플릿, 모델, 파서를 체인으로 연결\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "# 체인 실행\n",
    "response = chain.invoke({\"language\": \"Japanese\", \"text\": \"where is your pencil?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS 활용 Vector DB 구성 및 쿼리\n",
    "\n",
    "- FAISS는 벡터 유사성 검색 위한 라이브러리. OpenAIEmbeddings로 텍스트를 벡터로 변환해서 FAISS index에 저장.\n",
    "\n",
    "1. OpenAI Embedding model로 Vector Embedding 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. FAISS index 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "# FAISS 인덱스 생성\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Vector DB에 문서 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8272b84f-4dd4-48b6-ad66-dbad910a501b',\n",
       " 'dea5ae55-711e-46d5-92d4-7cacc38ce443',\n",
       " '954bddf1-34e2-4df8-9d90-0ce9f6250335',\n",
       " '9324fdd9-1192-49cb-86e0-ad68435bcaf9']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from uuid import uuid4\n",
    "\n",
    "# 문서 생성\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain을 사용해 프로젝트를 구축하고 있습니다!\", metadata={\"source\": \"tweet\"}),\n",
    "    Document(page_content=\"내일 날씨는 맑고 따뜻할 예정입니다.\", metadata={\"source\": \"news\"}),\n",
    "    Document(page_content=\"오늘 아침에는 팬케이크와 계란을 먹었어요.\", metadata={\"source\": \"personal\"}),\n",
    "    Document(page_content=\"주식 시장이 경기 침체 우려로 하락 중입니다.\", metadata={\"source\": \"news\"}),\n",
    "]\n",
    "\n",
    "# 고유 ID 생성 및 문서 추가\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Vector DB Query\n",
    "\n",
    "- 유사성 검색을 통해 특정 쿼리와 유사한 문서를 검색."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 내일 날씨는 맑고 따뜻할 예정입니다. [{'source': 'news'}]\n",
      "* 주식 시장이 경기 침체 우려로 하락 중입니다. [{'source': 'news'}]\n",
      "* [SIM=0.159] LangChain을 사용해 프로젝트를 구축하고 있습니다! [{'source': 'tweet'}]\n"
     ]
    }
   ],
   "source": [
    "# 기본 유사성 검색\n",
    "results = vector_store.similarity_search(\"내일 날씨는 어떨까요?\", k=2, filter={\"source\": \"news\"})\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "# 점수와 함께 유사성 검색\n",
    "results_with_scores = vector_store.similarity_search_with_score(\"LangChain에 대해 이야기해주세요.\", k=2, filter={\"source\": \"tweet\"})\n",
    "for res, score in results_with_scores:\n",
    "    print(f\"* [SIM={score:.3f}] {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chain에 FAISS 통합\n",
    "\n",
    "- RAG 체인을 구성하여 검색된 문서를 바탕으로 질문에 응답할 수 있도록 구성.\n",
    "\n",
    "1. FAISS를 Retriever로 변환해 RAG 체인에서 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. RAG Chain 생성\n",
    "\n",
    "- LangChain의 모델과 프롬프트를 연결하여 RAG 체인을 구성함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug Output: 내일 날씨는 어때?\n",
      "Debug Output: {'context': [Document(metadata={'source': 'news'}, page_content='내일 날씨는 맑고 따뜻할 예정입니다.')], 'question': '내일 날씨는 어때?'}\n",
      "Final Response:\n",
      "내일 날씨는 맑고 따뜻할 예정입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question using only the following context.\"),\n",
    "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "\n",
    "class DebugPassThrough(RunnablePassthrough):\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        output = super().invoke(*args, **kwargs)\n",
    "        print(\"Debug Output:\", output)\n",
    "        return output\n",
    "# 문서 리스트를 텍스트로 변환하는 단계 추가\n",
    "class ContextToText(RunnablePassthrough):\n",
    "    def invoke(self, inputs, config=None, **kwargs):  # config 인수 추가\n",
    "        # context의 각 문서를 문자열로 결합\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in inputs[\"context\"]])\n",
    "        return {\"context\": context_text, \"question\": inputs[\"question\"]}\n",
    "\n",
    "# RAG 체인에서 각 단계마다 DebugPassThrough 추가\n",
    "rag_chain_debug = {\n",
    "    \"context\": retriever,                    # 컨텍스트를 가져오는 retriever\n",
    "    \"question\": DebugPassThrough()        # 사용자 질문이 그대로 전달되는지 확인하는 passthrough\n",
    "} | DebugPassThrough() | ContextToText()| contextual_prompt | model\n",
    "\n",
    "# 질문 실행 및 각 단계 출력 확인\n",
    "response = rag_chain_debug.invoke(\"내일 날씨는 어때?\")\n",
    "print(\"Final Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS index의 저장 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 저장\n",
    "vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "# 저장된 인덱스 로드\n",
    "new_vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS DB 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = FAISS.from_texts([\"문서 1 내용\"], embeddings)\n",
    "db2 = FAISS.from_texts([\"문서 2 내용\"], embeddings)\n",
    "\n",
    "# 병합\n",
    "db1.merge_from(db2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Seok",
   "language": "python",
   "name": "seok"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
